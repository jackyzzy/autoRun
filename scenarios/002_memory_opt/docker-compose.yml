services:
  large-model-service:
    image: deepservice-llamacpp:v1
    container_name: large_model_service
    runtime: nvidia
    restart: unless-stopped  # 自动重启策略
    
    # GPU资源限制 - 只使用前2个GPU避免冲突
    environment:
      - CUDA_VISIBLE_DEVICES=0,1  # 指定使用GPU 0和1
      - NVIDIA_VISIBLE_DEVICES=0,1
      
    deploy:
      resources:
        limits:
          memory: 32g  # 限制内存使用32GB
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']  # 明确指定GPU设备
              capabilities: [gpu]
              
    # 健康检查
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # 给模型加载留足时间
      
    volumes:
      - /data:/data:ro  # 只读挂载数据目录
      
    ports:
      - "18001:18001"
      
    working_dir: /workspace/llama.cpp
    
    # 优化的启动命令
    command: >
      sh -c '
      echo "Starting llama-server with GPU devices: $$CUDA_VISIBLE_DEVICES" &&
      nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv &&
      /workspace/llama.cpp/build/bin/llama-server
      -m /data/model/DeepSeek-R1-32B-q4_k_m/deepseek-r1-distill-qwen-32b-q4_k_m.gguf
      --jinja --reasoning-format deepseek
      -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0
      -c 8192 -n 8192 --no-context-shift 
      --host 0.0.0.0 --port 18001
      '
      
    # 安全设置
    security_opt:
      - no-new-privileges:true
    read_only: false  # 模型加载需要写权限
    
    # 日志配置
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
