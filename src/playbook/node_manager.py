"""èŠ‚ç‚¹ç®¡ç†å™¨

ç®¡ç†å¤šä¸ªåˆ†å¸ƒå¼èŠ‚ç‚¹çš„SSHè¿æ¥å’Œæ“ä½œï¼Œæä¾›èŠ‚ç‚¹é…ç½®åŠ è½½ã€è¿æ¥ç®¡ç†ã€
å‘½ä»¤æ‰§è¡Œç­‰åŠŸèƒ½ã€‚æ”¯æŒèŠ‚ç‚¹æ± åŒ–ç®¡ç†å’Œè¿æ¥å¤ç”¨ã€‚

ä¸»è¦åŠŸèƒ½:
    - ä»YAMLé…ç½®æ–‡ä»¶åŠ è½½èŠ‚ç‚¹ä¿¡æ¯
    - ç®¡ç†SSHè¿æ¥æ± å’Œè¿æ¥ç”Ÿå‘½å‘¨æœŸ
    - æä¾›ç»Ÿä¸€çš„è¿œç¨‹å‘½ä»¤æ‰§è¡Œæ¥å£
    - æ”¯æŒå¹¶å‘æ“ä½œå¤šä¸ªèŠ‚ç‚¹
    - é›†æˆDocker Composeç‰ˆæœ¬é€‚é…
    - èŠ‚ç‚¹å¥åº·çŠ¶æ€ç›‘æ§å’Œè¿æ¥æµ‹è¯•

å…¸å‹ç”¨æ³•:
    >>> manager = NodeManager("config/nodes.yaml")
    >>> nodes = manager.get_nodes(enabled_only=True)
    >>> results = manager.execute_command("ls /", [node.name for node in nodes])
    >>> connectivity = manager.test_connectivity([node.name for node in nodes])
"""

import logging
import time
from typing import Dict, List, Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import yaml
from pathlib import Path
from datetime import datetime, timedelta

from ..utils.ssh_client import SSHClient, ssh_pool, SSHConnectionError, SSHExecutionError
from ..utils.docker_compose_adapter import DockerComposeAdapterManager, ComposeCommand
from ..utils.common import (
    setup_module_logger, load_yaml_config, validate_required_fields,
    retry_on_failure, ContextTimer
)
from ..utils.exception_handler import (
    with_exception_handling, node_operation, safe_execute, exception_context
)


class Node:
    """èŠ‚ç‚¹ä¿¡æ¯ç±»

    å°è£…å•ä¸ªèŠ‚ç‚¹çš„é…ç½®ä¿¡æ¯å’Œè¿æ¥å‚æ•°ã€‚

    Attributes:
        name: èŠ‚ç‚¹åç§°ï¼Œç”¨ä½œå”¯ä¸€æ ‡è¯†ç¬¦
        host: èŠ‚ç‚¹ä¸»æœºåœ°å€æˆ–IP
        username: SSHè¿æ¥ç”¨æˆ·å
        password: SSHè¿æ¥å¯†ç ï¼ˆå¯é€‰ï¼‰
        key_filename: SSHç§é’¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        port: SSHè¿æ¥ç«¯å£ï¼Œé»˜è®¤22
        tags: èŠ‚ç‚¹æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºåˆ†ç»„å’Œè¿‡æ»¤
        role: èŠ‚ç‚¹è§’è‰²ï¼Œå¦‚'master', 'worker'ç­‰
        enabled: æ˜¯å¦å¯ç”¨æ­¤èŠ‚ç‚¹
        docker_compose_path: Docker Composeæ–‡ä»¶è·¯å¾„
        work_dir: å·¥ä½œç›®å½•è·¯å¾„
        env_vars: ç¯å¢ƒå˜é‡æ˜ å°„

    Examples:
        >>> config = {
        ...     'host': '192.168.1.100',
        ...     'username': 'ubuntu',
        ...     'password': 'secret',
        ...     'enabled': True,
        ...     'tags': ['gpu', 'high-mem']
        ... }
        >>> node = Node('node1', config)
        >>> print(f"{node.name}: {node.host}")
        node1: 192.168.1.100
    """
    
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.host = config['host']
        self.username = config['username']
        self.password = config.get('password')
        self.key_filename = config.get('key_filename')
        self.port = config.get('port', 22)
        self.tags = config.get('tags', [])
        self.role = config.get('role', 'worker')
        self.enabled = config.get('enabled', True)
        
        # èŠ‚ç‚¹ç‰¹å®šé…ç½®
        self.docker_compose_path = config.get('docker_compose_path', '/opt/inference')
        self.work_dir = config.get('work_dir', '/opt/inference')

        # Docker Composeç‰ˆæœ¬é…ç½®ï¼ˆæ–°å¢ï¼‰
        self.docker_compose_version = config.get('docker_compose_version', 'auto')
        
    def get_ssh_client(self) -> SSHClient:
        """è·å–SSHå®¢æˆ·ç«¯"""
        return ssh_pool.get_connection(
            self.host, self.username, self.password, 
            self.key_filename, self.port
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'name': self.name,
            'host': self.host,
            'username': self.username,
            'port': self.port,
            'tags': self.tags,
            'role': self.role,
            'enabled': self.enabled,
            'docker_compose_path': self.docker_compose_path,
            'work_dir': self.work_dir,
            'docker_compose_version': self.docker_compose_version
        }


class NodeManager:
    """èŠ‚ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = None):
        self.logger = setup_module_logger("playbook.node_manager")
        self.nodes: Dict[str, Node] = {}
        self.config_file = config_file

        # è¿é€šæ€§ç¼“å­˜é…ç½®
        self.connectivity_cache: Dict[str, Tuple[bool, datetime]] = {}
        self.connectivity_cache_ttl = 300  # ç¼“å­˜TTL: 5åˆ†é’Ÿ
        self.connectivity_cache_enabled = True

        # åˆå§‹åŒ–Docker Composeé€‚é…å™¨ç®¡ç†å™¨
        self.compose_adapter_manager = DockerComposeAdapterManager(self.execute_command)

        if config_file and Path(config_file).exists():
            self.load_config(config_file)
    
    def load_config(self, config_file: str):
        """åŠ è½½èŠ‚ç‚¹é…ç½®"""
        with ContextTimer(self.logger, f"Loading node configuration from {config_file}"):
            config = load_yaml_config(config_file, required=True)

            self.nodes.clear()
            nodes_config = config.get('nodes', {})

            for node_name, node_config in nodes_config.items():
                # éªŒè¯å¿…éœ€å­—æ®µ
                validate_required_fields(
                    node_config,
                    ['host', 'username'],
                    f"node '{node_name}'"
                )

                node = Node(node_name, node_config)
                self.nodes[node_name] = node
                self.logger.debug(f"Loaded node: {node_name} ({node.host})")

            self.logger.info(f"Successfully loaded {len(self.nodes)} nodes")
    
    def add_node(self, name: str, host: str, username: str, **kwargs) -> Node:
        """æ·»åŠ èŠ‚ç‚¹"""
        config = {
            'host': host,
            'username': username,
            **kwargs
        }
        node = Node(name, config)
        self.nodes[name] = node
        self.logger.info(f"Added node: {name} ({host})")
        return node
    
    def remove_node(self, name: str) -> bool:
        """ç§»é™¤èŠ‚ç‚¹"""
        if name in self.nodes:
            del self.nodes[name]
            self.logger.info(f"Removed node: {name}")
            return True
        return False
    
    def get_node(self, name: str) -> Optional[Node]:
        """è·å–èŠ‚ç‚¹"""
        return self.nodes.get(name)
    
    def get_nodes(self, tags: List[str] = None, role: str = None, 
                  enabled_only: bool = True) -> List[Node]:
        """è·å–ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹åˆ—è¡¨"""
        nodes = []
        
        for node in self.nodes.values():
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨
            if enabled_only and not node.enabled:
                continue
                
            # æ£€æŸ¥æ ‡ç­¾
            if tags and not any(tag in node.tags for tag in tags):
                continue
                
            # æ£€æŸ¥è§’è‰²
            if role and node.role != role:
                continue
                
            nodes.append(node)
        
        return nodes
    
    def get_node_names(self, **kwargs) -> List[str]:
        """è·å–èŠ‚ç‚¹åç§°åˆ—è¡¨"""
        return [node.name for node in self.get_nodes(**kwargs)]
    
    @with_exception_handling("connectivity testing", return_on_error={})
    def test_connectivity(self, node_names: Optional[List[str]] = None, 
                         timeout: int = 10, force_refresh: bool = False) -> Dict[str, bool]:
        """æµ‹è¯•èŠ‚ç‚¹è¿æ¥æ€§ï¼ˆæ”¯æŒç¼“å­˜ä¼˜åŒ–ï¼‰
        
        Args:
            node_names: è¦æµ‹è¯•çš„èŠ‚ç‚¹åç§°åˆ—è¡¨
            timeout: è¿æ¥è¶…æ—¶æ—¶é—´
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            èŠ‚ç‚¹åç§°åˆ°è¿æ¥çŠ¶æ€çš„æ˜ å°„
        """
        if node_names is None:
            node_names = list(self.nodes.keys())
        
        current_time = datetime.now()
        results = {}
        nodes_to_test = []
        
        # æ£€æŸ¥ç¼“å­˜
        if self.connectivity_cache_enabled and not force_refresh:
            cache_hits = 0
            for node_name in node_names:
                if node_name in self.connectivity_cache:
                    cached_result, cached_time = self.connectivity_cache[node_name]
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    if (current_time - cached_time).total_seconds() < self.connectivity_cache_ttl:
                        results[node_name] = cached_result
                        cache_hits += 1
                        continue
                
                # ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼ŒåŠ å…¥å¾…æµ‹è¯•åˆ—è¡¨
                nodes_to_test.append(node_name)
            
            if cache_hits > 0:
                self.logger.debug(f"Connectivity cache hits: {cache_hits}/{len(node_names)} nodes")
        else:
            nodes_to_test = node_names
        
        # å¯¹éœ€è¦æµ‹è¯•çš„èŠ‚ç‚¹è¿›è¡Œè¿é€šæ€§æ£€æŸ¥
        if nodes_to_test:
            def test_node(node_name: str) -> Tuple[str, bool]:
                try:
                    node = self.nodes[node_name]
                    client = node.get_ssh_client()
                    with client.connection_context():
                        # æ‰§è¡Œç®€å•å‘½ä»¤æµ‹è¯•è¿æ¥
                        exit_code, _, _ = client.execute_command("echo 'test'", timeout=timeout)
                        return node_name, exit_code == 0
                except Exception as e:
                    self.logger.error(f"Connectivity test failed for {node_name}: {e}")
                    return node_name, False
            
            # å¹¶å‘æµ‹è¯•è¿æ¥
            with ThreadPoolExecutor(max_workers=min(len(nodes_to_test), 10)) as executor:
                future_to_node = {
                    executor.submit(test_node, node_name): node_name 
                    for node_name in nodes_to_test
                }
                
                for future in as_completed(future_to_node):
                    node_name, is_connected = future.result()
                    results[node_name] = is_connected
                    
                    # æ›´æ–°ç¼“å­˜
                    if self.connectivity_cache_enabled:
                        self.connectivity_cache[node_name] = (is_connected, current_time)
        
        # è®°å½•ç»“æœ
        connected_count = sum(results.values())
        total_nodes = len(node_names)
        cache_used = len(node_names) - len(nodes_to_test)
        
        if cache_used > 0:
            self.logger.info(f"Connectivity test: {connected_count}/{total_nodes} nodes connected "
                           f"({cache_used} from cache, {len(nodes_to_test)} tested)")
        else:
            self.logger.info(f"Connectivity test: {connected_count}/{total_nodes} nodes connected")
        
        return results
    
    def execute_command(self, command: str, node_names: Optional[List[str]] = None,
                       parallel: bool = True, timeout: int = 300,
                       stop_on_error: bool = False) -> Dict[str, Tuple[int, str, str]]:
        """åœ¨èŠ‚ç‚¹ä¸Šæ‰§è¡Œå‘½ä»¤"""
        if node_names is None:
            node_names = self.get_node_names(enabled_only=True)
        
        results = {}
        
        def execute_on_node(node_name: str) -> Tuple[str, Tuple[int, str, str]]:
            try:
                node = self.nodes[node_name]
                client = node.get_ssh_client()
                with client.connection_context():
                    result = client.execute_command(
                        command, timeout=timeout, check_exit_code=False
                    )
                    self.logger.info(f"Command executed on {node_name}: exit_code={result[0]}")
                    return node_name, result
            except Exception as e:
                self.logger.error(f"Command execution failed on {node_name}: {e}")
                return node_name, (-1, "", str(e))
        
        if parallel:
            # å¹¶å‘æ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=min(len(node_names), 10)) as executor:
                future_to_node = {
                    executor.submit(execute_on_node, node_name): node_name
                    for node_name in node_names
                }
                
                for future in as_completed(future_to_node):
                    node_name, result = future.result()
                    results[node_name] = result
                    
                    if stop_on_error and result[0] != 0:
                        self.logger.error(f"Command failed on {node_name}, stopping execution")
                        # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                        for f in future_to_node:
                            if not f.done():
                                f.cancel()
                        break
        else:
            # é¡ºåºæ‰§è¡Œ
            for node_name in node_names:
                node_name, result = execute_on_node(node_name)
                results[node_name] = result
                
                if stop_on_error and result[0] != 0:
                    self.logger.error(f"Command failed on {node_name}, stopping execution")
                    break
        
        return results
    
    @with_exception_handling("file upload", return_on_error={})
    def upload_file(self, local_path: str, remote_path: str,
                   node_names: Optional[List[str]] = None) -> Dict[str, bool]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°èŠ‚ç‚¹"""
        if node_names is None:
            node_names = self.get_node_names(enabled_only=True)
        
        results = {}
        
        def upload_to_node(node_name: str) -> Tuple[str, bool]:
            max_retries = 2
            retry_delay = 1

            for attempt in range(max_retries + 1):
                try:
                    node = self.nodes[node_name]
                    client = node.get_ssh_client()

                    if attempt > 0:
                        self.logger.info(f"Retrying file upload to {node_name} (attempt {attempt + 1})")
                        time.sleep(retry_delay)

                    with client.connection_context():
                        success = client.upload_file(local_path, remote_path)
                        if success:
                            if attempt > 0:
                                self.logger.info(f"File upload succeeded to {node_name} after {attempt + 1} attempts")
                            return node_name, True
                        else:
                            if attempt < max_retries:
                                self.logger.warning(f"File upload failed to {node_name}, will retry")
                            continue

                except Exception as e:
                    error_msg = str(e)
                    if attempt < max_retries:
                        self.logger.warning(f"File upload failed to {node_name} (attempt {attempt + 1}): {error_msg}, will retry")
                    else:
                        self.logger.error(f"File upload failed to {node_name} after {max_retries + 1} attempts: {error_msg}")

            return node_name, False
        
        # å¹¶å‘ä¸Šä¼ 
        with ThreadPoolExecutor(max_workers=min(len(node_names), 5)) as executor:
            future_to_node = {
                executor.submit(upload_to_node, node_name): node_name
                for node_name in node_names
            }
            
            for future in as_completed(future_to_node):
                node_name, success = future.result()
                results[node_name] = success
        
        success_count = sum(results.values())
        self.logger.info(f"File upload: {success_count}/{len(node_names)} nodes succeeded")
        
        return results
    
    def download_files(self, remote_path: str, local_base_path: str,
                      node_names: Optional[List[str]] = None) -> Dict[str, bool]:
        """ä»èŠ‚ç‚¹ä¸‹è½½æ–‡ä»¶"""
        if node_names is None:
            node_names = self.get_node_names(enabled_only=True)
        
        results = {}
        local_base = Path(local_base_path)
        local_base.mkdir(parents=True, exist_ok=True)
        
        def download_from_node(node_name: str) -> Tuple[str, bool]:
            try:
                node = self.nodes[node_name]
                client = node.get_ssh_client()
                
                # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºå­ç›®å½•
                local_path = local_base / node_name / Path(remote_path).name
                local_path.parent.mkdir(parents=True, exist_ok=True)
                
                with client.connection_context():
                    success = client.download_file(remote_path, str(local_path))
                    return node_name, success
            except Exception as e:
                self.logger.error(f"File download failed from {node_name}: {e}")
                return node_name, False
        
        # å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=min(len(node_names), 5)) as executor:
            future_to_node = {
                executor.submit(download_from_node, node_name): node_name
                for node_name in node_names
            }
            
            for future in as_completed(future_to_node):
                node_name, success = future.result()
                results[node_name] = success
        
        success_count = sum(results.values())
        self.logger.info(f"File download: {success_count}/{len(node_names)} nodes succeeded")
        
        return results
    
    @with_exception_handling("scenario env file upload", return_on_error={})
    def upload_scenario_env_file(self, env_file_path: str, scenario_name: str,
                                node_names: Optional[List[str]] = None) -> Dict[str, bool]:
        """å¢å¼ºç‰ˆscenarioç¯å¢ƒå˜é‡æ–‡ä»¶ä¸Šä¼ 
        
        Args:
            env_file_path: æœ¬åœ°ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„
            scenario_name: scenarioåç§°ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            node_names: ç›®æ ‡èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™ä¸Šä¼ åˆ°æ‰€æœ‰å¯ç”¨çš„èŠ‚ç‚¹
            
        Returns:
            èŠ‚ç‚¹åç§°åˆ°ä¸Šä¼ ç»“æœçš„æ˜ å°„
        """
        if not env_file_path or not Path(env_file_path).exists():
            self.logger.error(f"Environment file not found: {env_file_path}")
            return {}
        
        if node_names is None:
            node_names = self.get_node_names(enabled_only=True)
        
        # è®°å½•æœ¬åœ°æ–‡ä»¶ä¿¡æ¯
        local_file_info = Path(env_file_path).stat()
        self.logger.info(f"Uploading .env file for scenario '{scenario_name}': {env_file_path} ({local_file_info.st_size} bytes)")
        
        results = {}
        
        def upload_env_to_node(node_name: str) -> Tuple[str, bool]:
            start_time = time.time()
            
            try:
                # è¯¦ç»†æ­¥éª¤æ—¥å¿—
                self.logger.info(f"[{node_name}] Step 1: Establishing connection...")
                node = self.nodes[node_name]
                client = node.get_ssh_client()
                
                # ğŸ”§ å¢å¼º: æ£€æŸ¥è·¯å¾„ä¸€è‡´æ€§å¹¶é€‰æ‹©æœ€ä½³ä¸Šä¼ è·¯å¾„
                self.logger.info(f"[{node_name}] Step 2: Determining target directory...")
                if node.work_dir != node.docker_compose_path:
                    self.logger.warning(f"Path inconsistency on {node_name}: work_dir({node.work_dir}) != docker_compose_path({node.docker_compose_path})")
                    self.logger.info(f"Using docker_compose_path for {node_name}: {node.docker_compose_path}")
                    target_dir = node.docker_compose_path
                else:
                    target_dir = node.work_dir
                
                remote_env_path = f"{target_dir}/.env"
                self.logger.info(f"[{node_name}] Target path: {remote_env_path}")
                
                with client.connection_context():
                    # ğŸ”§ å¢å¼º: ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                    self.logger.info(f"[{node_name}] Step 3: Preparing remote directory...")
                    mkdir_start = time.time()
                    mkdir_result = client.execute_command(f"mkdir -p {target_dir}", check_exit_code=False)
                    mkdir_duration = time.time() - mkdir_start
                    self.logger.info(f"[{node_name}] Directory creation: {mkdir_duration:.1f}s (exit_code: {mkdir_result[0]})")
                    
                    # ğŸ”§ å¢å¼º: å¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    self.logger.info(f"[{node_name}] Step 4: Backing up existing file...")
                    backup_start = time.time()
                    backup_cmd = f"test -f {remote_env_path} && cp {remote_env_path} {remote_env_path}.bak || true"
                    backup_result = client.execute_command(backup_cmd, check_exit_code=False)
                    backup_duration = time.time() - backup_start
                    self.logger.info(f"[{node_name}] Backup operation: {backup_duration:.1f}s (exit_code: {backup_result[0]})")
                    
                    # ä¸Šä¼ ç¯å¢ƒå˜é‡æ–‡ä»¶
                    self.logger.info(f"[{node_name}] Step 5: Starting file upload...")
                    upload_start = time.time()
                    success = client.upload_file(env_file_path, remote_env_path)
                    upload_duration = time.time() - upload_start
                    
                    if success:
                        self.logger.info(f"[{node_name}] Upload completed in {upload_duration:.1f}s")
                        
                        # è®¾ç½®æ­£ç¡®çš„æ–‡ä»¶æƒé™
                        self.logger.info(f"[{node_name}] Step 6: Setting file permissions...")
                        chmod_start = time.time()
                        chmod_result = client.execute_command(f"chmod 644 {remote_env_path}", check_exit_code=False)
                        chmod_duration = time.time() - chmod_start
                        self.logger.info(f"[{node_name}] Permission setting: {chmod_duration:.1f}s (exit_code: {chmod_result[0]})")
                        
                        # ğŸ”§ å¢å¼º: ç«‹å³éªŒè¯ä¸Šä¼ ç»“æœ
                        self.logger.info(f"[{node_name}] Step 7: Verifying upload...")
                        verify_start = time.time()
                        verify_result = client.execute_command(f"test -f {remote_env_path} && wc -c {remote_env_path}", check_exit_code=False)
                        verify_duration = time.time() - verify_start
                        
                        if verify_result[0] == 0:
                            remote_size = int(verify_result[1].strip().split()[0])
                            self.logger.info(f"[{node_name}] Size verification: {verify_duration:.1f}s (local: {local_file_info.st_size}, remote: {remote_size})")
                            
                            if remote_size == local_file_info.st_size:
                                # ğŸ”§ å¢å¼º: éªŒè¯æ–‡ä»¶å¯è¯»æ€§å’Œå†…å®¹
                                self.logger.info(f"[{node_name}] Step 8: Content verification...")
                                content_start = time.time()
                                content_check = client.execute_command(f"head -1 {remote_env_path}", check_exit_code=False)
                                content_duration = time.time() - content_start
                                
                                if content_check[0] == 0:
                                    first_line = content_check[1].strip()
                                    self.logger.info(f"[{node_name}] Content check: {content_duration:.1f}s")
                                    self.logger.debug(f"Remote .env first line on {node_name}: {first_line[:50]}...")
                                
                                total_duration = time.time() - start_time
                                self.logger.info(f"âœ… [{node_name}] Upload successful in {total_duration:.1f}s total")
                                return node_name, True
                            else:
                                total_duration = time.time() - start_time
                                self.logger.error(f"âŒ [{node_name}] File size mismatch after {total_duration:.1f}s: local={local_file_info.st_size}, remote={remote_size}")
                                return node_name, False
                        else:
                            total_duration = time.time() - start_time
                            self.logger.error(f"âŒ [{node_name}] Failed to verify uploaded file after {total_duration:.1f}s")
                            return node_name, False
                    else:
                        total_duration = time.time() - start_time
                        self.logger.error(f"âŒ [{node_name}] Upload failed after {upload_duration:.1f}s (total: {total_duration:.1f}s)")
                        return node_name, False
                        
            except Exception as e:
                total_duration = time.time() - start_time
                self.logger.error(f"âŒ [{node_name}] Upload failed after {total_duration:.1f}s with exception: {e}")
                import traceback
                self.logger.debug(f"Full traceback: {traceback.format_exc()}")
                return node_name, False
        
        # æ·»åŠ æ€»ä½“è¶…æ—¶æ§åˆ¶
        import signal
        import time
        
        def timeout_handler(signum, frame):
            self.logger.error("File upload timeout, forcing cleanup...")
            ssh_pool.close_all()
            raise TimeoutError("File upload timeout after 300 seconds")
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼ˆä»…åœ¨æ”¯æŒçš„ç³»ç»Ÿä¸Šï¼‰
        old_handler = None
        try:
            old_handler = signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(300)  # 5åˆ†é’Ÿæ€»è¶…æ—¶
        except (AttributeError, OSError):
            self.logger.warning("Signal-based timeout not available, using manual monitoring")
        
        try:
            # ä¼˜åŒ–: ä¸Šä¼ å‰å¼ºåˆ¶æ¸…ç†SSHè¿æ¥æ± 
            self.logger.info("Clearing SSH connections before upload...")
            ssh_pool.close_all()
            
            # ä¼˜åŒ–: å‡å°‘å¹¶å‘åº¦ï¼Œé¿å…è¿æ¥å†²çª
            max_workers = min(len(node_names), 2)  # ä»5å‡åˆ°2
            self.logger.debug(f"Using {max_workers} concurrent workers for upload")
            
            # æ ¹æ®èŠ‚ç‚¹æ•°é‡é€‰æ‹©ä¸Šä¼ ç­–ç•¥
            if len(node_names) <= 2:
                # å°è§„æ¨¡é¡ºåºä¸Šä¼ ï¼Œæ›´ç¨³å®š
                self.logger.info("Using sequential upload for better reliability")
                for node_name in node_names:
                    try:
                        self.logger.info(f"Uploading to {node_name}...")
                        start_time = time.time()
                        result = upload_env_to_node(node_name)
                        duration = time.time() - start_time
                        results[result[0]] = result[1]
                        
                        if result[1]:
                            self.logger.info(f"âœ… {node_name}: Upload completed in {duration:.1f}s")
                        else:
                            self.logger.error(f"âŒ {node_name}: Upload failed after {duration:.1f}s")
                            # å¤±è´¥æ—¶ç«‹å³æ¸…ç†è¿æ¥
                            self._cleanup_node_connection(node_name)
                            
                    except Exception as e:
                        self.logger.error(f"Upload failed for {node_name}: {e}")
                        results[node_name] = False
                        self._cleanup_node_connection(node_name)
            else:
                # å¹¶å‘ä¸Šä¼ ï¼ˆèŠ‚ç‚¹è¾ƒå¤šæ—¶ï¼‰
                self.logger.info(f"Using concurrent upload with {max_workers} workers")
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_to_node = {
                        executor.submit(upload_env_to_node, node_name): node_name
                        for node_name in node_names
                    }
                    
                    for future in as_completed(future_to_node):
                        node_name, success = future.result()
                        results[node_name] = success
                        
                        if not success:
                            # å¤±è´¥æ—¶æ¸…ç†è¿æ¥
                            self._cleanup_node_connection(node_name)
                            
        finally:
            # æ¢å¤ä¿¡å·å¤„ç†å™¨
            if old_handler is not None:
                try:
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    signal.signal(signal.SIGALRM, old_handler)
                except (AttributeError, OSError):
                    pass
        
        success_count = sum(results.values())
        failed_nodes = [name for name, success in results.items() if not success]
        
        if success_count == len(node_names):
            self.logger.info(f"âœ… Environment file upload for scenario '{scenario_name}': {success_count}/{len(node_names)} nodes succeeded")
        else:
            self.logger.error(f"âŒ Environment file upload for scenario '{scenario_name}': {success_count}/{len(node_names)} nodes succeeded. Failed nodes: {failed_nodes}")
        
        return results
    
    def verify_env_file_on_nodes(self, scenario_name: str, node_names: Optional[List[str]] = None) -> Dict[str, Dict[str, Any]]:
        """éªŒè¯èŠ‚ç‚¹ä¸Šçš„ç¯å¢ƒå˜é‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ­£ç¡®
        
        Args:
            scenario_name: scenarioåç§°ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            node_names: è¦éªŒè¯çš„èŠ‚ç‚¹åç§°åˆ—è¡¨
            
        Returns:
            èŠ‚ç‚¹åç§°åˆ°éªŒè¯ç»“æœçš„æ˜ å°„
        """
        if node_names is None:
            node_names = self.get_node_names(enabled_only=True)
        
        results = {}
        
        def verify_env_on_node(node_name: str) -> Tuple[str, Dict[str, Any]]:
            result = {
                'exists': False,
                'readable': False,
                'size': 0,
                'error': None
            }
            
            try:
                node = self.nodes[node_name]
                client = node.get_ssh_client()
                
                remote_env_path = f"{node.work_dir}/.env"
                
                with client.connection_context():
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    exit_code, _, _ = client.execute_command(f"test -f {remote_env_path}", check_exit_code=False)
                    result['exists'] = exit_code == 0
                    
                    if result['exists']:
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è¯»
                        exit_code, _, _ = client.execute_command(f"test -r {remote_env_path}", check_exit_code=False)
                        result['readable'] = exit_code == 0
                        
                        # è·å–æ–‡ä»¶å¤§å°
                        exit_code, stdout, _ = client.execute_command(f"stat -c %s {remote_env_path}", check_exit_code=False)
                        if exit_code == 0:
                            result['size'] = int(stdout.strip())
                    
            except Exception as e:
                result['error'] = str(e)
                self.logger.error(f"Failed to verify .env file on {node_name}: {e}")
            
            return node_name, result
        
        # å¹¶å‘éªŒè¯
        with ThreadPoolExecutor(max_workers=min(len(node_names), 5)) as executor:
            future_to_node = {
                executor.submit(verify_env_on_node, node_name): node_name
                for node_name in node_names
            }
            
            for future in as_completed(future_to_node):
                node_name, result = future.result()
                results[node_name] = result
        
        # è®°å½•éªŒè¯ç»“æœ
        valid_count = sum(1 for r in results.values() if r['exists'] and r['readable'])
        self.logger.info(f"Environment file verification for scenario '{scenario_name}': {valid_count}/{len(node_names)} nodes have valid .env files")
        
        return results
    
    def get_node_status(self, node_names: Optional[List[str]] = None) -> Dict[str, Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯"""
        if node_names is None:
            node_names = self.get_node_names(enabled_only=True)
        
        status_commands = [
            "uptime",
            "df -h /",
            "free -h",
            "docker ps --format 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}'",
        ]
        
        results = {}
        
        def get_status(node_name: str) -> Tuple[str, Dict[str, Any]]:
            try:
                node = self.nodes[node_name]
                client = node.get_ssh_client()
                status = {'connected': False, 'info': {}}
                
                with client.connection_context():
                    status['connected'] = True
                    
                    for cmd in status_commands:
                        try:
                            exit_code, stdout, stderr = client.execute_command(
                                cmd, timeout=30, check_exit_code=False
                            )
                            if exit_code == 0:
                                status['info'][cmd] = stdout.strip()
                            else:
                                status['info'][cmd] = f"Error: {stderr.strip()}"
                        except Exception as e:
                            status['info'][cmd] = f"Failed: {str(e)}"
                
                return node_name, status
                
            except Exception as e:
                self.logger.error(f"Failed to get status for {node_name}: {e}")
                return node_name, {'connected': False, 'error': str(e)}
        
        # å¹¶å‘è·å–çŠ¶æ€
        with ThreadPoolExecutor(max_workers=min(len(node_names), 10)) as executor:
            future_to_node = {
                executor.submit(get_status, node_name): node_name
                for node_name in node_names
            }
            
            for future in as_completed(future_to_node):
                node_name, status = future.result()
                results[node_name] = status
        
        return results
    
    def _cleanup_node_connection(self, node_name: str):
        """æ¸…ç†æŒ‡å®šèŠ‚ç‚¹çš„è¿æ¥"""
        try:
            node = self.nodes.get(node_name)
            if node:
                client = node.get_ssh_client()
                if client:
                    client.disconnect()
                    self.logger.debug(f"Cleaned up connection for {node_name}")
        except Exception as e:
            self.logger.warning(f"Failed to cleanup connection for {node_name}: {e}")
    
    def close_all_connections(self):
        """å…³é—­æ‰€æœ‰SSHè¿æ¥"""
        ssh_pool.close_all()
        self.logger.info("Closed all SSH connections")
    
    def clear_connectivity_cache(self, node_names: Optional[List[str]] = None):
        """æ¸…ç†è¿é€šæ€§ç¼“å­˜
        
        Args:
            node_names: è¦æ¸…ç†çš„èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™æ¸…ç†å…¨éƒ¨ç¼“å­˜
        """
        if node_names is None:
            self.connectivity_cache.clear()
            self.logger.info("Cleared all connectivity cache")
        else:
            for node_name in node_names:
                self.connectivity_cache.pop(node_name, None)
            self.logger.info(f"Cleared connectivity cache for nodes: {node_names}")
    
    def get_connectivity_cache_stats(self) -> Dict[str, Any]:
        """è·å–è¿é€šæ€§ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        current_time = datetime.now()
        valid_entries = 0
        expired_entries = 0
        
        for node_name, (result, cache_time) in self.connectivity_cache.items():
            if (current_time - cache_time).total_seconds() < self.connectivity_cache_ttl:
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            'enabled': self.connectivity_cache_enabled,
            'ttl_seconds': self.connectivity_cache_ttl,
            'total_entries': len(self.connectivity_cache),
            'valid_entries': valid_entries,
            'expired_entries': expired_entries,
            'cache_hit_potential': f"{(valid_entries / len(self.nodes) * 100):.1f}%" if self.nodes else "0%"
        }
    
    def set_connectivity_cache_ttl(self, ttl_seconds: int):
        """è®¾ç½®è¿é€šæ€§ç¼“å­˜TTL
        
        Args:
            ttl_seconds: ç¼“å­˜æœ‰æ•ˆæ—¶é—´ï¼ˆç§’ï¼‰
        """
        old_ttl = self.connectivity_cache_ttl
        self.connectivity_cache_ttl = ttl_seconds
        self.logger.info(f"Updated connectivity cache TTL: {old_ttl}s -> {ttl_seconds}s")
    
    def enable_connectivity_cache(self, enabled: bool = True):
        """å¯ç”¨æˆ–ç¦ç”¨è¿é€šæ€§ç¼“å­˜
        
        Args:
            enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        old_state = self.connectivity_cache_enabled
        self.connectivity_cache_enabled = enabled
        
        if not enabled:
            self.clear_connectivity_cache()
        
        self.logger.info(f"Connectivity cache: {'enabled' if enabled else 'disabled'} (was {'enabled' if old_state else 'disabled'})")
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹ç®¡ç†å™¨æ‘˜è¦ä¿¡æ¯"""
        total_nodes = len(self.nodes)
        enabled_nodes = len(self.get_nodes(enabled_only=True))
        
        roles = {}
        tags = set()
        
        for node in self.nodes.values():
            roles[node.role] = roles.get(node.role, 0) + 1
            tags.update(node.tags)
        
        return {
            'total_nodes': total_nodes,
            'enabled_nodes': enabled_nodes,
            'disabled_nodes': total_nodes - enabled_nodes,
            'roles': roles,
            'tags': sorted(list(tags)),
            'connection_pool_size': ssh_pool.get_connection_count(),
            'connectivity_cache': self.get_connectivity_cache_stats()
        }

    def build_compose_command(self, node_name: str, command_type: str, **kwargs) -> ComposeCommand:
        """
        ä¸ºæŒ‡å®šèŠ‚ç‚¹æ„å»ºDocker Composeå‘½ä»¤

        Args:
            node_name: èŠ‚ç‚¹åç§°
            command_type: å‘½ä»¤ç±»å‹ (up/down/stop/logsç­‰)
            **kwargs: å‘½ä»¤å‚æ•°

        Returns:
            æ„å»ºçš„å‘½ä»¤å¯¹è±¡
        """
        node = self.get_node(node_name)
        if not node:
            raise ValueError(f"Node not found: {node_name}")

        return self.compose_adapter_manager.build_command(
            node_name=node_name,
            command_type=command_type,
            forced_version=node.docker_compose_version,
            **kwargs
        )

    def get_compose_version_info(self, node_names: Optional[List[str]] = None) -> Dict[str, Dict[str, Any]]:
        """
        è·å–èŠ‚ç‚¹çš„Docker Composeç‰ˆæœ¬ä¿¡æ¯

        Args:
            node_names: èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™è·å–æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯

        Returns:
            èŠ‚ç‚¹ç‰ˆæœ¬ä¿¡æ¯å­—å…¸
        """
        if node_names is None:
            node_names = list(self.nodes.keys())

        # æ„å»ºå¼ºåˆ¶ç‰ˆæœ¬æ˜ å°„
        forced_versions = {}
        for node_name in node_names:
            node = self.get_node(node_name)
            if node:
                forced_versions[node_name] = node.docker_compose_version

        return self.compose_adapter_manager.detect_all_versions(node_names, forced_versions)

    def execute_compose_command(self, node_names: List[str], command_type: str,
                               timeout: int = 300, **kwargs) -> Dict[str, Tuple[int, str, str]]:
        """
        åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡ŒDocker Composeå‘½ä»¤

        Args:
            node_names: èŠ‚ç‚¹åç§°åˆ—è¡¨
            command_type: å‘½ä»¤ç±»å‹
            timeout: è¶…æ—¶æ—¶é—´
            **kwargs: å‘½ä»¤å‚æ•°

        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        results = {}

        for node_name in node_names:
            try:
                node = self.get_node(node_name)
                if not node:
                    results[node_name] = (-1, "", f"Node not found: {node_name}")
                    continue

                # æ„å»ºé€‚é…çš„å‘½ä»¤
                compose_cmd = self.build_compose_command(node_name, command_type, **kwargs)

                # åˆ‡æ¢åˆ°å·¥ä½œç›®å½•å¹¶æ‰§è¡Œ
                full_cmd = f"cd {node.docker_compose_path} && {compose_cmd.full_cmd}"

                self.logger.info(f"Executing compose command on {node_name}: {compose_cmd.full_cmd}")
                cmd_results = self.execute_command(full_cmd, [node_name], timeout=timeout)

                node_result = cmd_results.get(node_name, (-1, "", "No result"))
                results[node_name] = node_result

                if node_result[0] == 0:
                    self.logger.info(f"Compose command succeeded on {node_name}")
                else:
                    self.logger.error(f"Compose command failed on {node_name}: {node_result[2]}")

            except Exception as e:
                self.logger.error(f"Error executing compose command on {node_name}: {e}")
                results[node_name] = (-1, "", str(e))

        return results