version: '3.8'

services:
  # P服务 - 主推理服务
  p-1:
    image: "${INFERENCE_IMAGE}:${IMAGE_TAG}"
    container_name: "${CONTAINER_PREFIX}-p-1"
    restart: unless-stopped
    environment:
      - SERVICE_TYPE=${PRIMARY_SERVICE_TYPE}
      - GPU_MEMORY=${GPU_MEMORY}
      - MODEL_PATH=${MODEL_PATH}
      - REPLICAS=${REPLICAS}
    ports:
      - "${INFERENCE_HOST_PORT}:${SERVICE_PORT}"
    volumes:
      - "${DATA_VOLUME_PATH}:/data:ro"
      - "${LOG_VOLUME_PATH}:/app/logs"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${GPU_DEVICE_IDS}']
              capabilities: [gpu]
    networks:
      - inference-network

  # D服务 - 数据处理服务  
  d-1:
    image: "${DATA_PROCESSOR_IMAGE}:${DATA_PROCESSOR_TAG}"
    container_name: "${CONTAINER_PREFIX}-d-1"
    restart: unless-stopped
    environment:
      - SERVICE_TYPE=${DATA_SERVICE_TYPE}
      - BATCH_SIZE=${BATCH_SIZE}
    ports:
      - "${DATA_HOST_PORT}:${DATA_SERVICE_PORT}"
    volumes:
      - "${DATA_VOLUME_PATH}:/data:ro"
      - "${CACHE_VOLUME_PATH}:/app/cache"
    depends_on:
      - p-1
    networks:
      - inference-network

networks:
  inference-network:
    driver: bridge